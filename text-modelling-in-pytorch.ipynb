{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'train.csv', 'test.csv', 'embeddings']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "d4a8ac9629af8551d575ee2efe68c947061f1b62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import time\n",
    "pd.set_option('max_colwidth',400)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"F-score is ill-defined and being set to 0.0 due to no predicted samples.\")\n",
    "import re\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "92ba587a359eb4fba0f68b65af301dc27eb938e7"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "sub = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "39bbdd2b2bb964c7ba6cce9afc76e40b6b703c32"
   },
   "outputs": [],
   "source": [
    "max_features = 120000\n",
    "tk = Tokenizer(lower = True, filters='', num_words=max_features)\n",
    "full_text = list(train['question_text'].values) + list(test['question_text'].values)\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "56fd04c10417abb480f1457e0c914ccac5d8c8d4"
   },
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "# Clean the text\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x.lower()))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "# Clean numbers\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "# Clean speelings\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: replace_typical_misspell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "9f138daf5672eaee439669799eff63ce020a1cb9"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['question_text'].fillna('missing'))\n",
    "test_tokenized = tk.texts_to_sequences(test['question_text'].fillna('missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "955284ed70935b9fc33346e45e207216c9878be6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\n",
    "plt.yscale('log');\n",
    "plt.title('Distribution of question text length in characters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "2267e8c120a797e54f773944ba9e08360228fc44"
   },
   "outputs": [],
   "source": [
    "max_len = 72\n",
    "maxlen = 72\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "7617c0cf09558772a23bcaca17440e1e0a31f425"
   },
   "outputs": [],
   "source": [
    "y_train = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "bc1c79eae86176fea6372d3578f8a1c8c602f56e"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=10).split(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "70eb6b3884f10bb589efc7cc5ec416c8fc1faf47"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "embedding_path = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore'))\n",
    "# all_embs = np.stack(embedding_index.values())\n",
    "# emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std = -0.005838499, 0.48782197\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "fdbc9f00db4869b8c3236b15c44b9baa219c1e4f"
   },
   "outputs": [],
   "source": [
    "embedding_path = \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore') if len(o)>100)\n",
    "# all_embs = np.stack(embedding_index.values())\n",
    "# emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std = -0.0053247833, 0.49346462\n",
    "embedding_matrix1 = np.random.normal(emb_mean, emb_std, (nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix1[i] = embedding_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "7535f283b499d31ba62b3b029dcf7426e75ac482"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.mean([embedding_matrix, embedding_matrix1], axis=0)\n",
    "del embedding_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "53c26c69de8ec0aa8f3d3612fea5b53b2819df06"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        hidden_size = 128\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm_attention = Attention(hidden_size*2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size*2, maxlen)\n",
    "        \n",
    "        self.linear = nn.Linear(1024, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        \n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "306d6014e7201c1997154d4d691799dad4b70493"
   },
   "outputs": [],
   "source": [
    "m = NeuralNet()\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, validate=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # scheduler = CosineAnnealingLR(optimizer, T_max=5)\n",
    "    # scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    valid = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n",
    "    best_score = -np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        model.eval()\n",
    "        \n",
    "        valid_preds = np.zeros((x_val_fold.size(0)))\n",
    "        \n",
    "        if validate:\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                y_pred = model(x_batch).detach()\n",
    "\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            search_result = threshold_search(y_val.cpu().numpy(), valid_preds)\n",
    "\n",
    "            val_f1, val_threshold = search_result['f1'], search_result['threshold']\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} best_t={:.2f} \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_f1, val_threshold, elapsed_time))\n",
    "        else:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "    \n",
    "    valid_preds = np.zeros((x_val_fold.size(0)))\n",
    "    \n",
    "    avg_val_loss = 0.\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    print('Validation loss: ', avg_val_loss)\n",
    "\n",
    "    test_preds = np.zeros((len(test_loader.dataset)))\n",
    "    \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "    # scheduler.step()\n",
    "    \n",
    "    return valid_preds, test_preds#, test_preds_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "5b785db95fe5353e1bad7fc14c6ca864226bafcc"
   },
   "outputs": [],
   "source": [
    "x_test_cuda = torch.tensor(X_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "batch_size = 512\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "12bc43d30c9cb070d74e529fca3c12de547ffdeb"
   },
   "outputs": [],
   "source": [
    "seed=1029\n",
    "\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "f08f72fec551feb0555e98a1044706acdb2ec20d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/5 \t loss=0.1324 \t time=314.56s\n",
      "Epoch 2/5 \t loss=0.1151 \t time=314.54s\n",
      "Epoch 3/5 \t loss=0.1085 \t time=312.72s\n",
      "Epoch 4/5 \t loss=0.1016 \t time=310.76s\n",
      "Epoch 5/5 \t loss=0.0947 \t time=306.06s\n",
      "Validation loss:  0.10253760519329473\n",
      "Fold 2\n",
      "Epoch 1/5 \t loss=0.1301 \t time=286.81s\n",
      "Epoch 2/5 \t loss=0.1148 \t time=274.99s\n",
      "Epoch 3/5 \t loss=0.1080 \t time=275.38s\n",
      "Epoch 4/5 \t loss=0.1016 \t time=275.89s\n",
      "Epoch 5/5 \t loss=0.0940 \t time=275.35s\n",
      "Validation loss:  0.10098556330468403\n",
      "Fold 3\n",
      "Epoch 1/5 \t loss=0.1295 \t time=274.72s\n",
      "Epoch 2/5 \t loss=0.1141 \t time=275.51s\n",
      "Epoch 3/5 \t loss=0.1070 \t time=274.86s\n",
      "Epoch 4/5 \t loss=0.1005 \t time=276.37s\n",
      "Epoch 5/5 \t loss=0.0935 \t time=275.64s\n",
      "Validation loss:  0.10163183126680334\n",
      "Fold 4\n",
      "Epoch 1/5 \t loss=0.1296 \t time=276.19s\n",
      "Epoch 2/5 \t loss=0.1143 \t time=275.43s\n",
      "Epoch 3/5 \t loss=0.1080 \t time=275.44s\n",
      "Epoch 4/5 \t loss=0.1013 \t time=276.02s\n",
      "Epoch 5/5 \t loss=0.0949 \t time=275.83s\n",
      "Validation loss:  0.10333259634346614\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros(len(train))\n",
    "test_preds = np.zeros((len(test), len(splits)))\n",
    "n_epochs = 5\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):    \n",
    "    x_train_fold = torch.tensor(X_train[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    x_val_fold = torch.tensor(X_train[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    seed_everything(seed + i)\n",
    "    model = NeuralNet()\n",
    "    model.cuda()\n",
    "\n",
    "    valid_preds_fold, test_preds_fold = train_model(model,\n",
    "                                                                           x_train_fold, \n",
    "                                                                           y_train_fold, \n",
    "                                                                           x_val_fold, \n",
    "                                                                           y_val_fold, validate=False)\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds[:, i] = test_preds_fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "7bb038915f17750995398d294d028da5a4d7d0c7"
   },
   "outputs": [],
   "source": [
    "search_result = threshold_search(y_train, train_preds)\n",
    "sub['prediction'] = test_preds.mean(1) > search_result['threshold']\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
